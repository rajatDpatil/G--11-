# Install correct versions of required libraries
!pip install torch torchaudio torchvision
!pip install numpy==1.26.4 pandas==1.5.3 networkx==2.8.8
!pip install matplotlib seaborn opencv-python pytesseract
!pip install tts==0.22.0 gtts gradio

# Verify installed versions
import numpy as np
import pandas as pd
import networkx as nx
import torch
import torchaudio
import torchvision
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import pytesseract
import TTS
from gtts import gTTS
import gradio as gr
import os
from IPython.display import Audio, display

print(f"‚úÖ NumPy Version: {np.__version__}")
print(f"‚úÖ Pandas Version: {pd.__version__}")
print(f"‚úÖ NetworkX Version: {nx.__version__}")
print(f"‚úÖ Torch Version: {torch.__version__}")
print(f"‚úÖ Torchaudio Version: {torchaudio.__version__}")
print(f"‚úÖ TTS Version: {TTS.__version__}")
print(f"‚úÖ Gradio Version: {gr.__version__}")

from google.colab import drive
drive.mount('/content/drive')

DATASET_PATH = "/content/drive/MyDrive/Multilingual_Audio_Dataset"

import os

# Define dataset path again to make sure it's correct
DATASET_PATH = "/content/drive/MyDrive/Multilingual_Audio_Dataset"

# Verify that files exist and have the correct format
audio_files = [f for f in os.listdir(DATASET_PATH) if f.endswith(".mp3")]

if len(audio_files) > 0:
    print(f"Found {len(audio_files)} audio files in dataset:")
    print(audio_files)
else:
    print("No audio files found in dataset! Check file format and path.")

    #Visualizating the loaded dataset
import os
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Dataset Path
DATASET_PATH = "/content/drive/MyDrive/Multilingual_Audio_Dataset"

# List of 20 languages in the dataset
languages = ["English", "Hindi", "Marathi", "Bengali", "French", "German", "Spanish", "Chinese", "Arabic", "Japanese",
             "Russian", "Portuguese", "Italian", "Turkish", "Dutch", "Swedish", "Greek", "Polish", "Korean", "Tamil"]

# Mapping files to languages (assuming they are named 1.mp3, 2.mp3, ..., 20.mp3)
audio_files = sorted([f for f in os.listdir(DATASET_PATH) if f.endswith(".mp3")])
language_audio_map = dict(zip(languages, audio_files))

# Dictionary to store durations
durations = {}

# Load each audio file and get duration
for lang, file in language_audio_map.items():
    file_path = os.path.join(DATASET_PATH, file)
    try:
        y, sr = librosa.load(file_path, sr=None)
        duration = librosa.get_duration(y=y, sr=sr)
        durations[lang] = duration
    except Exception as e:
        print(f"Error processing {file}: {e}")

#Plot 1: Language Distribution (Bar Chart)
plt.figure(figsize=(12, 5))
plt.bar(durations.keys(), durations.values(), color='skyblue')
plt.xlabel("Languages")
plt.ylabel("Audio Duration (seconds)")
plt.xticks(rotation=45)
plt.title("Audio Duration Per Language")
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

#Plot 2: Histogram of Audio Durations
plt.figure(figsize=(8, 5))
plt.hist(durations.values(), bins=6, color="purple", alpha=0.7)
plt.xlabel("Duration (seconds)")
plt.ylabel("Number of Audios")
plt.title("Distribution of Audio Durations")
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

!pip install SpeechRecognition

!pip install pydub

import os
import librosa
import soundfile as sf

# Create a folder for voice references
voice_ref_path = "/content/drive/MyDrive/voice_references"
os.makedirs(voice_ref_path, exist_ok=True)
print(f"Created voice references folder at: {voice_ref_path}")

# Define source MP3 files from your dataset
# Use different audio files for different references
voice_sources = {
    "male": [
        "/content/drive/MyDrive/Multilingual_Audio_Dataset/18.mp3",  # Original male reference
        "/content/drive/MyDrive/Multilingual_Audio_Dataset/19.mp3",   # Second male voice
        "/content/drive/MyDrive/Multilingual_Audio_Dataset/13.mp3"   # Third male voice
    ],
    "female": [
        "/content/drive/MyDrive/Multilingual_Audio_Dataset/1.mp3",   # Original female reference
        "/content/drive/MyDrive/Multilingual_Audio_Dataset/2.mp3",   # Second female voice
        "/content/drive/MyDrive/Multilingual_Audio_Dataset/10.mp3"    # Third female voice
    ]
}

# Convert all references
for gender, sources in voice_sources.items():
    for i, source_path in enumerate(sources, 1):
        # Define output path (adding number suffix for additional references)
        suffix = "" if i == 1 else str(i)
        output_path = os.path.join(voice_ref_path, f"{gender}_reference{suffix}.wav")

        # Convert MP3 to WAV
        print(f"Converting {source_path} to {output_path}...")
        try:
            y, sr = librosa.load(source_path, sr=None)
            sf.write(output_path, y, sr)
            print(f"‚úÖ Created {gender} voice reference {i}")
        except Exception as e:
            print(f"‚ùå Error creating {gender} reference {i}: {e}")

print("Voice references creation completed!")

import cv2
import pytesseract
import speech_recognition as sr
import librosa
import soundfile as sf

# Function to handle user input: Text, Image, or Audio
def process_user_input(input_type, user_input, language):
    processed_text = None  # To store extracted text

    if input_type == "Text":
        processed_text = user_input  # Direct text input

    elif input_type == "Image":
        # Extract text from image using pytesseract
        image = cv2.imread(user_input)
        if image is None:
            print("Error: Could not read image file!")
            return None
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        processed_text = pytesseract.image_to_string(gray)

    elif input_type == "Audio":
        # Convert MP3 to WAV (if necessary) and then transcribe
        if user_input.endswith(".mp3"):
            wav_path = user_input.replace(".mp3", ".wav")
            audio, sr = librosa.load(user_input, sr=None)
            sf.write(wav_path, audio, sr)
            user_input = wav_path


        recognizer = sr.Recognizer()
        with sr.AudioFile(user_input) as source:
            audio_data = recognizer.record(source)
            try:
                processed_text = recognizer.recognize_google(audio_data, language=language)
            except sr.UnknownValueError:
                print("Speech Recognition could not understand the audio")
                processed_text = None
            except sr.RequestError:
                print("Could not request results from Google Speech Recognition")
                processed_text = None

    return processed_text

    import re

def clean_text(text):
    """
    Clean text: remove unwanted characters, fix spacing, and apply basic NLP corrections.
    """
    text = text.lower()
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[^a-zA-Z0-9.,!? ]", "", text)
    return text.strip()

# Example: Preprocessing test
sample_text = "Hello!!  How    are   you?? üòä"
cleaned_text = clean_text(sample_text)
print("Cleaned Text:", cleaned_text)  # get Output:"hello! how are you?" without the emoji

!pip install Pillow

!mkdir -p /content/sample_datasets/images
!mkdir -p /content/sample_datasets/audio

from google.colab import files

# For images
uploaded_images = files.upload()
for filename in uploaded_images.keys():
    !mv "{filename}" "/content/sample_datasets/images/{filename}"
# For audio
uploaded_audio = files.upload()
for filename in uploaded_audio.keys():
    !mv "{filename}" "/content/sample_datasets/audio/{filename}"

    !ls -la /content/sample_datasets/images
!ls -la /content/sample_datasets/audio

import os
import torch
import torchaudio
import numpy as np
import gradio as gr
from PIL import Image
import transformers
from transformers import pipeline
import sys
import glob

# Install espeak-ng which is required
!apt-get update && apt-get install -y espeak-ng

# Set environment variable to accept license
os.environ["COQUI_TOS_AGREED"] = "1"

#import TTS
from TTS.api import TTS


def initialize_tts_model():
    try:

        print("Loading TTS model...")
        model = TTS("tts_models/multilingual/multi-dataset/your_tts")


        languages = ["en", "es", "fr", "de", "it", "pt", "pl", "tr", "ru", "nl", "cs", "ar", "zh-cn", "hi"]
        print(f"Available languages: {languages}")
        print("TTS Model loaded successfully!")
        return model, languages
    except Exception as e:
        print(f"Error initializing multilingual TTS model: {str(e)}")
        try:

            model = TTS("tts_models/en/ljspeech/fast_pitch")
            print("Fallback TTS Model loaded successfully!")
            return model, ["en"]
        except Exception as e2:
            print(f"Error initializing fallback TTS model: {str(e2)}")
            print("Could not initialize any TTS model. Please check your installation.")
            return None, ["en"]

# Load speaker
def load_speaker_samples():
    print("Loading speaker samples...")
    speaker_samples = {
        "male": [],
        "female": []
    }


    dataset_path = "/content/drive/MyDrive/Multilingual_Audio_Dataset"

    # Check if the directory exists
    if not os.path.exists(dataset_path):
        print(f"Warning: Dataset path {dataset_path} not found")
        os.makedirs(dataset_path, exist_ok=True)
        return speaker_samples

    # Load samples
    male_samples = glob.glob(os.path.join(dataset_path, "*male*.mp3")) + \
                   glob.glob(os.path.join(dataset_path, "m_*.mp3"))


    female_samples = glob.glob(os.path.join(dataset_path, "*female*.mp3")) + \
                     glob.glob(os.path.join(dataset_path, "f_*.mp3"))


    if len(male_samples) == 0:
        print("No male samples found, creating placeholder")
        male_samples = ["/content/sample_datasets/audio/male_placeholder.mp3"]

    if len(female_samples) == 0:
        print("No female samples found, creating placeholder")
        female_samples = ["/content/sample_datasets/audio/female_placeholder.mp3"]

    speaker_samples["male"] = male_samples[:10]
    speaker_samples["female"] = female_samples[:10]
    print(f"Loaded {len(speaker_samples['male'])} male and {len(speaker_samples['female'])} female speaker samples")
    return speaker_samples


def create_sample_datasets():
    print("Creating sample datasets for testing...")
    base_dir = "/content/sample_datasets/"
    os.makedirs(os.path.join(base_dir, "images"), exist_ok=True)
    os.makedirs(os.path.join(base_dir, "audio"), exist_ok=True)
    print(f"Sample dataset structure created at {base_dir}")


    image_files = [f for f in os.listdir(os.path.join(base_dir, "images"))
                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

    audio_files = [f for f in os.listdir(os.path.join(base_dir, "audio"))
                  if f.lower().endswith(('.wav', '.mp3'))]

    # Create placeholder files if none exist
    if len(image_files) == 0:
        print("No images found, creating placeholder")
        with open(os.path.join(base_dir, "images", "placeholder.jpg"), "w") as f:
            f.write("placeholder")

    if len(audio_files) == 0:
        print("No audio files found, creating placeholders")
        with open(os.path.join(base_dir, "audio", "male_placeholder.mp3"), "w") as f:
            f.write("placeholder")
        with open(os.path.join(base_dir, "audio", "female_placeholder.mp3"), "w") as f:
            f.write("placeholder")

    print(f"Found {len(image_files)} images and {len(audio_files)} audio files")
    return base_dir

# Process text input
def process_text(text, language="en", speaker_type="male", speaker_wav=None):
    if tts_model is None:
        return None, "TTS model not initialized properly"

    try:

        output_path = "output_speech.wav"

        print(f"Processing text in language: {language}, Speaker type: {speaker_type}")


        if speaker_wav is None or speaker_wav == "":
            if speaker_type in speaker_samples and speaker_samples[speaker_type]:

                import random
                speaker_wav = random.choice(speaker_samples[speaker_type])
                print(f"Using {speaker_type} voice sample: {speaker_wav}")

        # Different models have different parameter requirements
        try:

            if speaker_wav and os.path.exists(speaker_wav):
                tts_model.tts_to_file(
                    text=text,
                    file_path=output_path,
                    language=language,
                    speaker_wav=speaker_wav
                )
            else:

                tts_model.tts_to_file(text=text, file_path=output_path, language=language)
        except Exception as e1:
            print(f"Error with parameters: {str(e1)}")

            tts_model.tts_to_file(text=text, file_path=output_path)

        return output_path, "Text processed successfully!"
    except Exception as e:
        return None, f"Error processing text: {str(e)}"

# Process image input
def process_image(image, language="en", speaker_type="male", speaker_wav=None):
    try:
        if image is None:
            return None, "No image provided!"


        caption = image_captioner(image)
        text = caption[0]['generated_text']

        print(f"Image caption: {text}")


        return process_text(text, language, speaker_type, speaker_wav)
    except Exception as e:
        return None, f"Error processing image: {str(e)}"

# Process audio input
def process_audio(audio_file, language="en", speaker_type="male", speaker_wav=None):
    try:
        if audio_file is None:
            return None, "No audio provided!"

        text = asr_model(audio_file)["text"]

        print(f"Transcribed text: {text}")

        return process_text(text, language, speaker_type, speaker_wav)
    except Exception as e:
        return None, f"Error processing audio: {str(e)}"

def main():
    global tts_model, image_captioner, asr_model, available_languages, speaker_samples

    # Initialize models
    tts_model, available_languages = initialize_tts_model()
    if tts_model is None:
        print("Failed to initialize TTS model. Please check your installation.")
        return

    print("Initializing image captioning model...")
    image_captioner = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")

    print("Initializing ASR model...")
    asr_model = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")


    dataset_dir = create_sample_datasets()


    speaker_samples = load_speaker_samples()


    with gr.Blocks(title="Zero-Shot TTS Model") as demo:
        gr.Markdown("# Zero-Shot Text, Image, and Audio to Speech Model")

        with gr.Tab("Text to Speech"):
            with gr.Row():
                text_input = gr.Textbox(
                    label="Enter Text",
                    value="Hello, this is a test of the zero-shot text to speech system.",
                    lines=5
                )

            with gr.Row():

                language_input = gr.Dropdown(
                    choices=available_languages,
                    value=available_languages[0],
                    label="Language",
                    allow_custom_value=True  #
                )


                speaker_type_input = gr.Radio(
                    choices=["male", "female"],
                    value="male",
                    label="Speaker Gender"
                )


                speaker_voice_input = gr.Audio(
                    type="filepath",
                    label="Upload custom voice reference (optional)"
                )

            text_submit = gr.Button("Generate Speech from Text")

            with gr.Row():
                text_output = gr.Audio(label="Generated Speech")
                text_status = gr.Textbox(label="Status")

            text_submit.click(
                fn=process_text,
                inputs=[text_input, language_input, speaker_type_input, speaker_voice_input],
                outputs=[text_output, text_status]
            )

        with gr.Tab("Image to Speech"):
            with gr.Row():
                image_input = gr.Image(type="pil", label="Upload Image")

            with gr.Row():
                img_language_input = gr.Dropdown(
                    choices=available_languages,
                    value=available_languages[0],
                    label="Language",
                    allow_custom_value=True
                )

                # Add speaker type selection for image tab
                img_speaker_type_input = gr.Radio(
                    choices=["male", "female"],
                    value="male",
                    label="Speaker Gender"
                )

                img_speaker_audio = gr.Audio(
                    type="filepath",
                    label="Upload custom voice reference (optional)"
                )

            image_submit = gr.Button("Generate Speech from Image")

            with gr.Row():
                image_output = gr.Audio(label="Generated Speech")
                image_status = gr.Textbox(label="Status")

            image_submit.click(
                fn=process_image,
                inputs=[image_input, img_language_input, img_speaker_type_input, img_speaker_audio],
                outputs=[image_output, image_status]
            )

        with gr.Tab("Audio to Speech"):
            with gr.Row():
                audio_input = gr.Audio(type="filepath", label="Upload Audio")

            with gr.Row():
                audio_language_input = gr.Dropdown(
                    choices=available_languages,
                    value=available_languages[0],
                    label="Language",
                    allow_custom_value=True
                )

                # Add speaker type selection for audio tab
                audio_speaker_type_input = gr.Radio(
                    choices=["male", "female"],
                    value="male",
                    label="Speaker Gender"
                )

                audio_speaker_input = gr.Audio(
                    type="filepath",
                    label="Upload custom voice reference (optional)"
                )

            audio_submit = gr.Button("Generate Speech from Audio")

            with gr.Row():
                audio_output = gr.Audio(label="Generated Speech")
                audio_status = gr.Textbox(label="Status")

            audio_submit.click(
                fn=process_audio,
                inputs=[audio_input, audio_language_input, audio_speaker_type_input, audio_speaker_input],
                outputs=[audio_output, audio_status]
            )


        gr.Markdown("## Examples")
        with gr.Accordion("About this model", open=False):
            gr.Markdown("""
            # Zero-Shot TTS Model

            This model combines several AI capabilities:
            1. **Text-to-Speech**: Direct conversion from text to speech
            2. **Image-to-Speech**: Captions an image, then converts that caption to speech
            3. **Audio-to-Speech**: Transcribes audio, then converts that text to speech

            All three modules support:
            - Multiple languages
            - Male and female voice options
            - Custom voice cloning with reference audio

            The model uses pre-trained components:
            - TTS: YourTTS multilingual model
            - Image Captioning: ViT-GPT2
            - ASR: Wav2Vec2
            """)

    # Launch the demo
    print("Launching Gradio interface...")
    demo.launch(share=True)

if __name__ == "__main__":
    main()
